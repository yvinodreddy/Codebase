# Quick Reference Card

## ğŸš€ One-Liners

```bash
# Basic usage
python cli_interface.py "Your prompt here"

# With Claude API
python cli_interface.py --claude "Your prompt"

# Interactive mode
python cli_interface.py --interactive

# Save to file
python cli_interface.py --output results.json "Your prompt"

# Run demo
python demo.py

# Run tests
python test_orchestration.py
```

## ğŸ Python Quick Start

```python
from master_orchestrator import MasterOrchestrator

orchestrator = MasterOrchestrator(min_confidence_score=96.0)
result = orchestrator.process("Your prompt")

print(f"Confidence: {result.confidence_score:.2f}%")
print(f"Success: {result.success}")
print(f"Output: {result.output}")
```

## ğŸ“Š Confidence Score Guide

| Score | Meaning | Action |
|-------|---------|--------|
| 96-100% | Excellent | Use with confidence |
| 90-95% | Good | Review output |
| <90% | Needs work | Auto-refined |

## ğŸ›¡ï¸ 7 Guardrail Layers

1. **Prompt Shields** - Jailbreak prevention
2. **Input Content** - Harmful content detection
3. **PHI Detection** - Privacy protection
4. **Terminology** - Medical accuracy
5. **Output Content** - Safe generation
6. **Groundedness** - Factual accuracy
7. **Compliance** - HIPAA & fact checking

## ğŸ”§ Common Commands

```bash
# Adjust confidence
python cli_interface.py --min-confidence 98.0 "prompt"

# Use specific model
python cli_interface.py --claude --model claude-3-opus-20240229 "prompt"

# Verbose logging
python cli_interface.py -v "prompt"

# Help
python cli_interface.py --help
```

## ğŸ“ File Structure

```
TestPrompt/
â”œâ”€â”€ agent_framework/          # 8 agent components
â”œâ”€â”€ guardrails/               # 5 guardrail files
â”œâ”€â”€ prompt_preprocessor.py    # Intent classification
â”œâ”€â”€ master_orchestrator.py    # Main coordinator
â”œâ”€â”€ claude_integration.py     # Claude SDK
â”œâ”€â”€ cli_interface.py          # CLI tool
â”œâ”€â”€ demo.py                   # Demonstration
â”œâ”€â”€ test_orchestration.py     # Tests
â”œâ”€â”€ config.yaml               # Configuration
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ *.md                      # Documentation
```

## ğŸ¯ Confidence Scoring Formula

```
Confidence = (
    Prompt Analysis Ã— 15% +
    Agent Execution Ã— 25% +
    Guardrails Ã— 30% +
    Iteration Efficiency Ã— 15% +
    Verification Ã— 15%
)

Target: 96-100%
```

## ğŸ’¡ Best Practices

1. âœ… Be specific in prompts
2. âœ… Provide context
3. âœ… Trust confidence scores â‰¥96%
4. âœ… Use appropriate thresholds
5. âœ… Review logs for insights

## ğŸ› Quick Troubleshooting

**Problem:** API key error
**Fix:** Export `ANTHROPIC_API_KEY` or use local mode

**Problem:** Low confidence
**Fix:** Be more specific or let system auto-refine

**Problem:** Slow processing
**Fix:** Normal for complex validation (5-10s)

## ğŸ“š Documentation Files

- `README.md` - Complete documentation
- `GETTING_STARTED.md` - Quick start guide
- `IMPLEMENTATION_SUMMARY.md` - What was built
- `QUICK_REFERENCE.md` - This file

## ğŸ”— Integration Examples

### Basic
```python
from master_orchestrator import MasterOrchestrator
orchestrator = MasterOrchestrator()
result = orchestrator.process("Explain AI")
```

### With Claude
```python
from claude_integration import ClaudeOrchestrator
orchestrator = ClaudeOrchestrator()
response = orchestrator.process("Explain AI")
print(response.response_text)
```

### Advanced
```python
orchestrator = MasterOrchestrator(
    min_confidence_score=98.0,
    max_refinement_iterations=5
)
result = orchestrator.process(
    prompt="Complex task",
    source_documents=["doc1.txt", "doc2.txt"]
)
```

## ğŸ“Š Statistics

```python
stats = orchestrator.get_statistics()
print(f"Success Rate: {stats['success_rate']:.2f}%")
print(f"Avg Confidence: {stats['average_confidence']:.2f}%")
```

## ğŸ® Interactive Mode Commands

```
ğŸ¤– Enter prompt: Your prompt here
ğŸ¤– Enter prompt: quit    # Exit
```

## âš™ï¸ Environment Variables

```bash
ANTHROPIC_API_KEY=your_key
CONTENT_SAFETY_ENDPOINT=endpoint
CONTENT_SAFETY_KEY=key
ENABLE_PROMPT_SHIELDS=true
LOG_LEVEL=INFO
```

## ğŸ§ª Testing

```bash
# Run all tests
python test_orchestration.py

# Run with pytest
pytest test_orchestration.py -v

# With coverage
pytest --cov=. --cov-report=html
```

## ğŸ“ˆ Success Metrics

- **Accuracy Target:** 96-100%
- **Success Rate:** >95%
- **Avg Processing:** 2-5 seconds
- **Guardrail Layers:** 7
- **Agent Components:** 8

## ğŸ¯ What This System Does

```
INPUT: Any text prompt
  â†“
[Preprocessing] â†’ Intent + Complexity
  â†“
[Guardrails 1-3] â†’ Input validation
  â†“
[Agent Execution] â†’ Smart processing
  â†“
[Guardrails 4-7] â†’ Output validation
  â†“
[QA Scoring] â†’ 96-100% confidence
  â†“
[Refinement] â†’ If needed
  â†“
OUTPUT: High-quality result
```

## ğŸ’» System Requirements

- Python 3.8+
- pip packages (see requirements.txt)
- Optional: Anthropic API key
- Optional: Azure Content Safety credentials

## ğŸ“ Learning Path

1. Run `demo.py`
2. Try `cli_interface.py` examples
3. Read `GETTING_STARTED.md`
4. Explore Python API
5. Customize `config.yaml`
6. Run `test_orchestration.py`

---

**Remember:** The system automatically refines until reaching 96-100% confidence. Trust the process! ğŸš€
