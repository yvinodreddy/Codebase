# Prometheus Configuration for SwarmCare
# Production-ready monitoring stack

server:
  global:
    scrape_interval: 15s
    scrape_timeout: 10s
    evaluation_interval: 15s

  retention: 30d

  resources:
    limits:
      cpu: 2000m
      memory: 8Gi
    requests:
      cpu: 1000m
      memory: 4Gi

  persistentVolume:
    enabled: true
    size: 100Gi
    storageClass: managed-premium

  service:
    type: ClusterIP

  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - prometheus.swarmcare.example.com
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus.swarmcare.example.com

alertmanager:
  enabled: true

  config:
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'slack-critical'
      routes:
        - match:
            severity: critical
          receiver: 'slack-critical'
        - match:
            severity: warning
          receiver: 'slack-warning'

    receivers:
      - name: 'slack-critical'
        slack_configs:
          - channel: '#alerts-critical'
            title: 'SwarmCare Critical Alert'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
      - name: 'slack-warning'
        slack_configs:
          - channel: '#alerts-warning'
            title: 'SwarmCare Warning'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  persistentVolume:
    enabled: true
    size: 10Gi

serverFiles:
  alerting_rules.yml:
    groups:
      - name: swarmcare_alerts
        interval: 30s
        rules:
          # High CPU usage
          - alert: HighCPUUsage
            expr: rate(container_cpu_usage_seconds_total{namespace="swarmcare"}[5m]) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage detected"
              description: "Pod {{ $labels.pod }} CPU usage is above 80%"

          # High memory usage
          - alert: HighMemoryUsage
            expr: container_memory_usage_bytes{namespace="swarmcare"} / container_spec_memory_limit_bytes{namespace="swarmcare"} > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage detected"
              description: "Pod {{ $labels.pod }} memory usage is above 90%"

          # Pod crash looping
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total{namespace="swarmcare"}[15m]) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Pod is crash looping"
              description: "Pod {{ $labels.pod }} has restarted {{ $value }} times"

          # Service down
          - alert: ServiceDown
            expr: up{job="swarmcare"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Service is down"
              description: "SwarmCare service {{ $labels.instance }} is down"

          # High error rate
          - alert: HighErrorRate
            expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "High error rate detected"
              description: "Error rate is {{ $value | humanizePercentage }}"

          # Database connection issues
          - alert: DatabaseConnectionIssues
            expr: postgresql_up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Database connection failed"
              description: "Cannot connect to PostgreSQL"

          # Redis connection issues
          - alert: RedisConnectionIssues
            expr: redis_up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Redis connection failed"
              description: "Cannot connect to Redis"

          # Disk space running low
          - alert: DiskSpaceLow
            expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Disk space running low"
              description: "Disk space is below 10% on {{ $labels.instance }}"

          # High response time
          - alert: HighResponseTime
            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High response time"
              description: "95th percentile response time is {{ $value }}s"

# ServiceMonitor for application metrics
serviceMonitors:
  - name: swarmcare
    selector:
      matchLabels:
        app.kubernetes.io/name: swarmcare
    endpoints:
      - port: http
        interval: 30s
        path: /metrics

# Additional scrape configs
extraScrapeConfigs: |
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
