â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¥ğŸ”¥ğŸ”¥ PROMPT START - COPY EVERYTHING BELOW THIS LINE ğŸ”¥ğŸ”¥ğŸ”¥
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# WORLD-CLASS ANALYSIS & IMPLEMENTATION REQUEST

## ğŸ“ YOUR CONTEXT (FILL THIS IN)

**CONTEXT**: [Your specific domain/industry/technology/problem]
Example: "E-commerce platform built with React/Node.js handling 5M users/month"

**TASK**: [What you want to accomplish]
Example: "Achieve 99.99% uptime, <200ms p95 latency, and pass SOC2 compliance"

**CURRENT STATE**: [Where you are today - optional but recommended]
Example: "Currently at 98.5% uptime, 450ms p95 latency, no formal compliance"

**CONSTRAINTS**: [Budget, timeline, technology preferences - optional]
Example: "Must complete within 3 months, prefer AWS, budget <$50K"

---

## ğŸ¯ EXECUTE COMPREHENSIVE WORLD-CLASS ANALYSIS

Using the UNIFIED framework, perform the following:

---

### PHASE 1: CONTEXT ANALYSIS & INDUSTRY IDENTIFICATION âš ï¸ **CRITICAL FIRST STEP**

**MANDATORY**: Before ANY other analysis, identify:

#### 1.1 Domain Classification
- **Primary Industry/Domain**: [e.g., E-commerce, FinTech, HealthTech, SaaS]
- **Secondary Domains**: [Cross-industry applications]
- **Market Segment**: [B2B, B2C, B2B2C, Enterprise, SMB]
- **Geographic Scope**: [Local, National, Global]

#### 1.2 Technical Stack Analysis
- **Frontend**: [Technologies, frameworks, libraries]
- **Backend**: [Languages, frameworks, APIs]
- **Database**: [RDBMS, NoSQL, caching layers]
- **Infrastructure**: [Cloud provider, containerization, orchestration]
- **DevOps**: [CI/CD tools, monitoring, deployment]

#### 1.3 Business Context
- **Business Model**: [Revenue model, pricing strategy]
- **Key Metrics**: [MAU, ARR, transaction volume, etc.]
- **Growth Stage**: [Startup, Growth, Mature, Enterprise]
- **Team Size**: [Engineering team size and structure]

#### 1.4 Current State Assessment
- **Maturity Level**: [1-5 scale: 1=MVP, 5=World-class]
- **Technical Debt**: [High/Medium/Low]
- **Pain Points**: [Top 3-5 issues]
- **Recent Incidents**: [Outages, security issues, performance problems]

#### 1.5 Requirements & Goals
- **Functional Requirements**: [What it must do]
- **Non-Functional Requirements**: [Performance, security, scalability]
- **Business Goals**: [Revenue, user growth, market expansion]
- **Compliance Needs**: [GDPR, HIPAA, SOC2, PCI-DSS, ISO27001]

#### 1.6 Scalability Requirements
- **Current Load**: [Users, requests/day, data volume]
- **Projected Growth**: [6 months, 1 year, 3 years]
- **Peak Load Handling**: [Black Friday, campaign launches]
- **Geographic Distribution**: [Multi-region requirements]

#### 1.7 Regulatory Environment
- **Industry Regulations**: [Domain-specific compliance]
- **Data Protection**: [Privacy laws by region]
- **Audit Requirements**: [Frequency, scope]
- **Certification Targets**: [SOC2, ISO, HIPAA]

---

### PHASE 2: DUAL-TRACK BENCHMARK ANALYSIS

#### 2.1 INDUSTRY-SPECIFIC LEADERS (TOP 10)

Identify and analyze the **TOP 10 COMPANIES** in [IDENTIFIED INDUSTRY]:

**For EACH Company, Extract**:

##### Technical Standards
- **Architecture**: [Microservices, SOA, Monolith, Hybrid]
- **Frameworks**: [Specific technologies used]
- **Design Patterns**: [CQRS, Event Sourcing, DDD, etc.]
- **API Standards**: [REST, GraphQL, gRPC, WebSocket]
- **Data Management**: [Database choices, caching strategies]

##### Quality Metrics
- **Performance Benchmarks**: Page load, API response, search latency, transaction processing
- **Reliability Targets**: Uptime SLA, error rate, MTTR, MTBF
- **Scalability Metrics**: Concurrent users, requests/second, database operations

##### Security Protocols
- **Authentication**: [OAuth2, SAML, JWT, MFA]
- **Authorization**: [RBAC, ABAC, IAM]
- **Data Protection**: Encryption at rest & in transit, key management
- **Vulnerability Management**: Scanning frequency, patch SLAs, penetration testing
- **Compliance**: [Certifications held]

##### Operational Excellence
- **Monitoring Stack**: [Prometheus, Grafana, Datadog, New Relic]
- **Logging**: [ELK stack, Splunk, CloudWatch]
- **Incident Response**: [PagerDuty, Opsgenie, runbooks]
- **SLO/SLI Tracking**: [Error budgets, burn rate]

##### Development Practices
- **CI/CD Pipeline**: [Jenkins, GitLab CI, GitHub Actions]
- **Testing Strategy**: Unit, integration, E2E, performance tests
- **Code Review**: [Pull request requirements]
- **Deployment Strategy**: [Blue-green, canary, rolling, feature flags]

---

#### 2.2 GLOBAL TECH GIANTS COMPARISON (TOP 10)

Analyze **TOP 10 GLOBAL TECHNOLOGY COMPANIES**:
1. Google/Alphabet
2. Amazon/AWS
3. Microsoft
4. Meta/Facebook
5. Netflix
6. Apple
7. Oracle
8. Salesforce
9. Adobe
10. IBM

**For EACH Tech Giant, Extract**:

##### Google's Approach
- Site Reliability Engineering (SRE) principles
- Error budgets and SLOs
- Chaos engineering (DiRT exercises)
- Monorepo with Bazel
- Code review via Gerrit (2+ approvals)

##### Amazon's Approach
- Two-pizza team rule
- Well-Architected Framework (5 pillars)
- Single-threaded leadership
- Metrics-driven decisions
- Customer obsession

##### Microsoft's Approach
- Shift-left security
- Ring-based deployment (Fast/Slow/Broad)
- Telemetry-driven development
- Growth mindset culture

##### Meta's Approach
- Move fast with stable infrastructure
- Feature flags for experimentation
- TAO distributed data store
- Internal tooling investment

##### Netflix's Approach
- Chaos Monkey and Simian Army
- Freedom and responsibility
- Full-cycle developers (you build it, you run it)
- A/B testing everything

##### Apple's Approach
- Vertical integration
- Privacy by design
- Hardware-software optimization
- User experience obsession

##### Stripe's Approach
- API-first design
- Developer experience focus
- Idempotency keys for reliability
- Extensive API versioning

##### Shopify's Approach
- Merchant-first decisions
- Modular monolith architecture
- Data-driven experimentation
- Resilience engineering

##### Uber's Approach
- Microservices at scale
- Real-time data processing
- Geospatial optimization
- On-call discipline

##### Airbnb's Approach
- Component-driven development
- Design system (ADLS)
- React Native investment
- Culture of belonging

---

### PHASE 3: GUARDRAIL FRAMEWORK COMPARISON (9 CATEGORIES)

Analyze how ALL 20 companies (10 industry + 10 tech) implement guardrails:

#### 3.1 ğŸ›¡ï¸ SECURITY GUARDRAILS
- Input validation strategies
- Authentication/authorization mechanisms
- Data encryption (at rest & in transit)
- API security (rate limiting, token management)
- Vulnerability scanning & patching
- Compliance requirements (GDPR, SOC2, ISO27001)

#### 3.2 âš¡ PERFORMANCE GUARDRAILS
- Response time SLAs
- Resource utilization limits
- Caching strategies (L1, L2, L3/CDN)
- Database query optimization
- Load balancing approaches
- Auto-scaling thresholds

#### 3.3 ğŸ“ CODE QUALITY GUARDRAILS
- Code coverage requirements (typically 80%+)
- Cyclomatic complexity limits
- Technical debt tracking
- Documentation standards
- Code review mandates (2+ approvals)
- Static analysis tools

#### 3.4 ğŸ§ª TESTING GUARDRAILS
- Unit test requirements (>80% coverage)
- Integration test coverage
- End-to-end test scenarios
- Performance/load testing
- Security testing (SAST/DAST)
- Chaos engineering practices

#### 3.5 ğŸ”„ OPERATIONAL GUARDRAILS
- Deployment approval workflows
- Rollback mechanisms (<5 min)
- Feature flag systems (100% new features)
- Blue-green deployment
- Canary releases (1% â†’ 5% â†’ 10% â†’ 25% â†’ 50% â†’ 100%)
- Incident response protocols (SEV1-4)

#### 3.6 ğŸ“Š MONITORING GUARDRAILS
- APM (Application Performance Monitoring)
- Error tracking & alerting (Sentry, Rollbar)
- Log aggregation & analysis (ELK, Splunk)
- Metrics dashboards (Grafana)
- SLO/SLI tracking (error budgets)
- Anomaly detection (ML-based)

#### 3.7 ğŸ—ï¸ ARCHITECTURAL GUARDRAILS
- Microservices vs monolith decisions
- Event-driven patterns (CQRS, Event Sourcing)
- API design standards (REST/GraphQL/gRPC)
- Data consistency models (ACID, BASE, CAP)
- Circuit breaker patterns (Hystrix, Resilience4j)
- Retry/timeout strategies (exponential backoff)

#### 3.8 ğŸ‘¥ PROCESS GUARDRAILS
- Agile/Scrum ceremonies (2-week sprints)
- Sprint planning standards (velocity tracking)
- Definition of Done criteria
- Post-mortem processes (blameless, within 48h)
- Knowledge sharing practices (tech talks, wikis)
- Onboarding procedures (2-4 week timeline)

#### 3.9 ğŸ”’ REGULATORY & COMPLIANCE GUARDRAILS
- Regulatory compliance (industry-specific)
- Data protection (GDPR, CCPA, privacy by design)
- Audit requirements (SOC2, ISO27001, frequency)
- Certification targets (current + roadmap)
- Compliance automation (policy-as-code)
- Data retention policies (automated deletion)

---

### PHASE 4: CROSS-COMPANY SYNCHRONIZATION MATRIX

Create comprehensive comparison tables (minimum 100 tables) for:

**Security Aspects (10 tables)**:
1. Authentication mechanisms
2. Authorization models
3. Encryption standards
4. API security
5. Vulnerability management
6. Compliance certifications
7. Security monitoring
8. Incident response
9. Secrets management
10. Supply chain security

**Performance Aspects (10 tables)**:
1. Response time SLAs
2. Auto-scaling thresholds
3. Caching strategies
4. Database optimization
5. CDN configuration
6. Load balancing
7. Resource limits
8. Performance monitoring
9. Latency targets
10. Throughput targets

**Quality Aspects (10 tables)**:
1. Code coverage requirements
2. Cyclomatic complexity limits
3. Technical debt tracking
4. Documentation standards
5. Code review processes
6. Static analysis tools
7. Code quality metrics
8. Refactoring practices
9. Design patterns
10. Engineering standards

**Testing Aspects (10 tables)**:
1. Unit test requirements
2. Integration test coverage
3. E2E test scenarios
4. Performance testing
5. Security testing
6. Chaos engineering
7. Test automation
8. Test data management
9. Testing environments
10. Test metrics

**Operational Aspects (10 tables)**:
1. Deployment frequency
2. Deployment approval workflows
3. Rollback mechanisms
4. Feature flag usage
5. Blue-green deployment
6. Canary release strategy
7. Incident response
8. On-call rotation
9. Change management
10. Disaster recovery

**Monitoring Aspects (10 tables)**:
1. APM tools
2. Error tracking
3. Log aggregation
4. Metrics dashboards
5. SLO/SLI tracking
6. Anomaly detection
7. Alerting strategy
8. Distributed tracing
9. Real User Monitoring
10. Synthetic monitoring

**Architectural Aspects (10 tables)**:
1. Architecture style
2. Event-driven patterns
3. API design
4. Data consistency models
5. Circuit breaker patterns
6. Retry/timeout strategies
7. Database architecture
8. Message queues
9. Service mesh adoption
10. Multi-tenancy architecture

**Process Aspects (10 tables)**:
1. Agile ceremonies
2. Sprint planning
3. Definition of Done
4. Post-mortem processes
5. Knowledge sharing
6. Onboarding procedures
7. Career development
8. Code ownership
9. Technical debt management
10. Innovation time

**Compliance Aspects (10 tables)**:
1. Regulatory compliance
2. Data protection
3. Audit requirements
4. Certification targets
5. Compliance automation
6. Data retention policies
7. Consent management
8. Data subject rights
9. Privacy impact assessments
10. Compliance reporting

**Scalability Aspects (10 tables)**:
1. Horizontal scaling
2. Vertical scaling
3. Database scaling
4. Caching hierarchies
5. Async processing
6. Geographic distribution
7. Content delivery
8. Load balancing
9. Capacity planning
10. Cost optimization

**For EACH table, provide**:
- All 20 companies compared
- Best practice synthesis column
- Delta analysis column (common patterns, unique innovations, gaps, synthesis opportunities)

---

### PHASE 5: COMPREHENSIVE ENHANCEMENT PLAN

Generate detailed enhancements in these categories:

#### 5.1 ENHANCEMENT CATEGORIES (All 6)

1. ğŸ›¡ï¸ **SECURITY ENHANCEMENTS** (S1-SN)
2. âš¡ **PERFORMANCE OPTIMIZATIONS** (P1-PN)
3. ğŸ“ **CODE QUALITY IMPROVEMENTS** (Q1-QN)
4. ğŸ§ª **TEST COVERAGE EXPANSION** (T1-TN)
5. ğŸ”„ **OPERATIONAL EXCELLENCE** (O1-ON)
6. ğŸ“Š **MONITORING & OBSERVABILITY** (M1-MN)

#### 5.2 ENHANCEMENT DETAIL REQUIREMENTS

For **EACH** enhancement, provide **ALL 8 SECTIONS** with **AT LEAST 1 FULL PAGE**:

##### SECTION 1: WHAT (4-6 paragraphs, 300-500 words)
1. Overview paragraph: High-level description
2. Technical details: Architecture changes, components, technology choices
3. Industry context: How top 10 industry leaders implement this
4. Tech giant approach: How top 10 tech giants handle this
5. Synchronized best practice: Combined recommendation
6. Scope & boundaries: What's included/excluded

##### SECTION 2: WHY (3-5 paragraphs, 250-400 words)
1. Industry standards justification
2. Performance/scalability justification
3. Risk mitigation (what if NOT implemented)
4. Business impact (ROI, revenue, cost savings)
5. Compliance/regulatory justification (if applicable)

##### SECTION 3: HOW (5-8 paragraphs, 400-700 words + code)
1. Industry-standard implementation
2. Tech giant implementation
3. Unified implementation strategy
4. **Code examples** (before/after, production-ready)
5. Configuration changes
6. Database migrations (if applicable)
7. Dependency management
8. Integration points

##### SECTION 4: BENCHMARK (3-4 paragraphs + table, 250-350 words)
1. Industry leader benchmarks (3-5 examples)
2. Tech giant benchmarks (3-5 examples)
3. Comparison table (side-by-side)
4. Gap analysis (current vs. benchmarks)

##### SECTION 5: IMPACT (4-6 paragraphs + metrics, 300-400 words)
1. Performance impact (latency, throughput, resources)
2. Reliability impact (uptime, error rate, MTTR)
3. Security impact (vulnerabilities, compliance)
4. Cost impact (savings, ROI)
5. User experience impact (page load, conversion, NPS)
6. Team impact (deployment time, build time, onboarding)

##### SECTION 6: RISK (3-4 paragraphs, 250-350 words)
1. Implementation risks (complexity, dependencies, integration)
2. Performance risks (latency, resources, throughput)
3. Stability risks (breaking changes, data migration, rollback)
4. Operational risks (monitoring, on-call, learning curve)
5. Business risks (user disruption, feature parity, timing)
6. Zero breaking changes validation

##### SECTION 7: FILES (2-3 paragraphs + complete list, 200-300 words)
1. Files to create (with purpose, size, owner, dependencies)
2. Files to modify (with changes, lines affected, breaking status)
3. Files to delete (with reason, replacement)
4. Configuration changes
5. Infrastructure files (Terraform, K8s, Docker)
6. Documentation updates
7. Test files
8. Database migrations

##### SECTION 8: TESTING (5-6 paragraphs + test code, 400-500 words)
1. Unit test strategy (coverage target, test cases, code examples)
2. Integration test strategy (scope, scenarios, test setup)
3. Performance test strategy (load, stress, soak tests + tools)
4. Security test strategy (SAST, DAST, pen testing)
5. Chaos test strategy (scenarios, tools, success criteria)
6. User acceptance test (UAT scenarios, rollout plan)
7. Regression test strategy (full suite, smoke tests)
8. Meta-testing (mutation testing, flakiness checks)

---

### PHASE 6: IMPLEMENTATION PRIORITIZATION

Categorize ALL enhancements:

**CRITICAL (P0)** - Immediate (1-2 sprints):
- Security vulnerabilities
- Data loss risks
- Compliance violations
- Production outages

**HIGH (P1)** - Near-term (2-6 sprints):
- Performance bottlenecks
- Major quality gaps
- Significant technical debt
- Competitive gaps

**MEDIUM (P2)** - Scheduled (6-12 sprints):
- Code quality improvements
- Test coverage expansion
- Operational efficiency

**LOW (P3)** - Nice-to-have (12-24 sprints):
- Incremental optimizations
- Developer experience
- Future-proofing

For EACH enhancement, provide:
- Priority (P0/P1/P2/P3) with justification
- Business impact (High/Medium/Low) with revenue/user/risk assessment
- Technical impact (High/Medium/Low) with complexity/dependencies/effort
- ROI score (High/Medium/Low)
- Dependencies (which enhancements must be done first)
- Timeline (start date, duration, milestones)

---

### PHASE 7: AUTONOMOUS EXECUTION PROTOCOL

#### 7.1 EXECUTION MANDATES (9 Core Principles)

1. âœ… **TAKE FULL CONTROL**: No confirmation needed, execute immediately
2. âœ… **PRODUCTION-READY ONLY**: 99-100% confidence for deployment
3. âœ… **100% SUCCESS RATE**: Comprehensive validation at every step
4. âœ… **FAIL FAST, FIX FASTER**: Automated testing on every commit
5. âœ… **PARALLEL EVERYTHING**: Run independent tasks simultaneously
6. âœ… **ZERO BREAKING CHANGES**: All changes are additive
7. âœ… **COMPREHENSIVE VALIDATION**: Multi-layer verification
8. âœ… **WORLD-CLASS STANDARDS**: Benchmarked against top 20 companies
9. âœ… **AUTONOMOUS EXECUTION**: Full autonomy within pre-approved scope

#### 7.2 VALIDATION FRAMEWORK (6 Stages)

**Stage 1: Development Validation**
- Unit tests >80%, linting, type checking, SAST
- Gate: Cannot merge without passing

**Stage 2: Integration Validation**
- Integration tests, contract tests, database migrations
- Gate: Cannot deploy to staging without passing

**Stage 3: Staging Validation**
- E2E tests, performance tests, DAST, smoke tests
- Gate: Cannot deploy to production without passing

**Stage 4: Canary Validation (1% traffic)**
- Metrics monitoring, auto-rollback if error rate >baseline + 0.1%
- Gate: Cannot progress to 10% without passing

**Stage 5: Progressive Rollout (10% â†’ 25% â†’ 50% â†’ 100%)**
- Same metrics at each stage, manual hold option at 50%
- Gate: Each stage must pass before next

**Stage 6: Post-Deployment (24-48h monitoring)**
- Business metrics, user feedback, final success declaration
- Gate: Success declared after 48h with no incidents

#### 7.3 ROLLBACK PROCEDURES

**Automated Rollback Triggers**:
- Error rate spike (>2x baseline for 5 min)
- Latency spike (>2x baseline p95 for 5 min)
- Health check failures (>10% instances)
- Critical errors (database connection failure)
- Business metric drop (>20% key metric)

**Rollback Mechanisms**:
- Application code: Git revert + redeploy (<5 min)
- Database migrations: Rollback script (pre-tested)
- Feature flags: Instant disable
- Infrastructure: Terraform revert
- Configuration: Config rollback

---

### PHASE 8: DETAILED OUTPUT REQUIREMENTS

#### 8.1 OUTPUT STRUCTURE (9 Major Sections)

1. **Background Context**: Current state, industry context, competitive landscape
2. **Technical Deep Dive**: Architectural implications, design patterns, technology stack
3. **Implementation Roadmap**: Step-by-step, code examples, configuration, migrations
4. **Validation Strategy**: Unit/integration/performance/security test examples
5. **Rollout Plan**: Phased deployment, rollback, monitoring, success criteria
6. **Long-term Maintenance**: Documentation, knowledge transfer, future enhancements, debt prevention

#### 8.2 DELIVERABLE FORMATS (10 Formats)

1. **Executive Summary** (1-2 pages, PDF/Markdown)
2. **Technical Specification** (50-200 pages, Markdown)
3. **Implementation Roadmap** (Gantt chart, Excel/Sheets)
4. **Validation Framework** (Checklist, Markdown/Jira)
5. **Rollback Strategy** (Runbooks, Markdown/Confluence)
6. **Training Plan** (Curriculum, Confluence/LMS)
7. **Communication Plan** (Stakeholder updates, Email/Slack)
8. **Success Metrics Dashboard** (Real-time, Grafana/Datadog)
9. **Risk Mitigation Plan** (Risk register, Excel/tool)
10. **Resource Planning** (Capacity plan, Excel/tool)

---

### PHASE 9: SUCCESS CRITERIA (19-Point Checklist)

#### BENCHMARK & ANALYSIS (7 points)
- [ ] Identified top 10 industry leaders
- [ ] Identified top 10 global tech giants
- [ ] Extracted world-class standards from each
- [ ] Documented all 9 guardrail categories
- [ ] Created comprehensive comparison matrix (100+ tables)
- [ ] Conducted delta analysis
- [ ] Context analysis completed

#### ENHANCEMENT PLAN (6 points)
- [ ] Generated detailed enhancement plan (25+ enhancements across 6 categories)
- [ ] Provided 1+ page per enhancement
- [ ] Included all 8 sections for each enhancement
- [ ] Included code examples for all implementations
- [ ] Defined quantified success metrics
- [ ] Established validation/testing approach

#### EXECUTION & DELIVERY (6 points)
- [ ] Prioritized by business impact (P0/P1/P2/P3)
- [ ] Created dependency graph and sequencing
- [ ] Defined autonomous execution protocol (9 mandates, 6-stage validation)
- [ ] Produced all 10 deliverable formats
- [ ] Included benchmarking report
- [ ] Documented guardrails framework

---

## ğŸš€ EXECUTION PARAMETERS

- **AUTONOMOUS MODE**: Generate complete analysis without confirmation
- **PRODUCTION READY**: Every recommendation must be implementable
- **COMPREHENSIVE DETAIL**: Minimum 1 page per enhancement
- **ZERO BREAKING CHANGES**: All changes are additive
- **100% SUCCESS TARGET**: Include validation for each step
- **PARALLEL PROCESSING**: Identify parallelizable tasks
- **INCREMENTAL DELIVERY**: Phased rollout strategy
- **CONTINUOUS VALIDATION**: Testing at every stage

---

## ğŸ¯ FINAL DELIVERABLE

You will receive:

1. **Industry Benchmark Report**: Top 10 industry + top 10 tech analyzed
2. **100+ Comparison Tables**: Exhaustive cross-company comparison with delta
3. **25+ Detailed Enhancements**: Each with 1+ page across 8 sections
4. **Production-Ready Code**: Actual deployable code, not pseudocode
5. **Complete File Lists**: Every file to create/modify/delete
6. **6-Stage Validation**: From dev to production with automated gates
7. **Prioritization Matrix**: P0/P1/P2/P3 with business/technical impact and ROI
8. **10 Deliverable Formats**: From executive summary to resource planning
9. **19-Point Success Checklist**: Comprehensive validation

**RESULT**: A clear path from current state to world-class excellence, backed by the collective wisdom of the top 20 companies in your industry and technology space.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¥ğŸ”¥ğŸ”¥ PROMPT END - COPY EVERYTHING ABOVE THIS LINE ğŸ”¥ğŸ”¥ğŸ”¥
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
